{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK7RrgR0LbFESbkOwd7plw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hphung188/UW_ML_Fraud/blob/main/fraud_pipeline_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Features of the Pipeline:\n",
        "\n",
        "Data Loading & Preprocessing: Reads data, handles missing values, and encodes categorical features.\n",
        "\n",
        "Feature Selection: Uses variance threshold, chi-square, mutual information, and ExtraTreesClassifier.\n",
        "\n",
        "Resampling Techniques: Supports NearMiss (undersampling) and SMOTE (oversampling).\n",
        "\n",
        "Model Training: Includes Decision Trees, Random Forest, XGBoost, and Neural Networks.\n",
        "\n",
        "Hyperparameter Tuning: Implements GridSearchCV and RandomizedSearchCV.\n",
        "\n",
        "Model Evaluation: Uses Confusion Matrix, ROC-AUC, and Recall scores."
      ],
      "metadata": {
        "id": "HX3hSMW0Le8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdSEi7y-LRlW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class FraudDetectionPipeline:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.preprocessed_data = None\n",
        "        self.selected_features = None\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads data from CSV file.\"\"\"\n",
        "        self.df = pd.read_csv(self.filepath)\n",
        "        print(\"Data Loaded Successfully.\")\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Handles missing values, scales numerical data, and encodes categorical features.\"\"\"\n",
        "        # Handling missing values\n",
        "        missing_features = ['prev_address_months_count', 'current_address_months_count',\n",
        "                            'intended_balcon_amount', 'bank_months_count', 'session_length_in_minutes']\n",
        "        for feature in missing_features:\n",
        "            self.df.loc[self.df[feature] < 0, feature] = np.nan\n",
        "\n",
        "        # One-hot encoding categorical features\n",
        "        categorical_features = [col for col in self.df.columns if self.df[col].dtype == 'O']\n",
        "        self.df = pd.get_dummies(self.df, columns=categorical_features)\n",
        "\n",
        "        # Splitting features and target variable\n",
        "        X = self.df.drop(columns=['fraud_bool'])\n",
        "        y = self.df['fraud_bool']\n",
        "\n",
        "        # Scaling numeric features\n",
        "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        preprocessor = ColumnTransformer([('scaled', MinMaxScaler(), numeric_features)], remainder='passthrough')\n",
        "        X_scaled = preprocessor.fit_transform(X)\n",
        "        X_scaled = pd.DataFrame(X_scaled, columns=numeric_features + list(set(X.columns) - set(numeric_features)))\n",
        "\n",
        "        self.preprocessed_data = (X_scaled, y)\n",
        "        print(\"Data Preprocessing Completed.\")\n",
        "\n",
        "    def feature_selection(self):\n",
        "        \"\"\"Applies multiple feature selection techniques and selects the most relevant features.\"\"\"\n",
        "        X_scaled, y = self.preprocessed_data\n",
        "\n",
        "        # Variance Threshold\n",
        "        selector = VarianceThreshold()\n",
        "        X_scaled = X_scaled.loc[:, selector.fit(X_scaled).get_support()]\n",
        "\n",
        "        # Chi-Square Test\n",
        "        chi2_selector = SelectKBest(chi2, k=15).fit(X_scaled, y)\n",
        "        chi2_features = X_scaled.columns[chi2_selector.get_support()]\n",
        "\n",
        "        # Mutual Information\n",
        "        mi_selector = SelectKBest(mutual_info_classif, k=15).fit(X_scaled, y)\n",
        "        mi_features = X_scaled.columns[mi_selector.get_support()]\n",
        "\n",
        "        # ExtraTreesClassifier Feature Importance\n",
        "        extra = ExtraTreesClassifier(n_estimators=50, random_state=0).fit(X_scaled, y)\n",
        "        extra_features = X_scaled.columns[extra.feature_importances_ > 0.02]\n",
        "\n",
        "        # Final Feature Selection\n",
        "        selected_features = set(chi2_features) | set(mi_features) | set(extra_features)\n",
        "        self.selected_features = list(selected_features)\n",
        "        self.preprocessed_data = (X_scaled[self.selected_features], y)\n",
        "        print(\"Feature Selection Completed. Selected Features:\", self.selected_features)\n",
        "\n",
        "    def balance_data(self, method='undersampling'):\n",
        "        \"\"\"Balances dataset using NearMiss (undersampling) or SMOTE (oversampling).\"\"\"\n",
        "        X, y = self.preprocessed_data\n",
        "\n",
        "        if method == 'undersampling':\n",
        "            sampler = NearMiss(sampling_strategy=0.1, n_jobs=-1)\n",
        "        else:\n",
        "            sampler = SMOTE(random_state=42)\n",
        "\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)\n",
        "        print(f\"Data Balanced using {method}. Training size: {len(self.X_train)}, Test size: {len(self.X_test)}\")\n",
        "\n",
        "    def train_classifier(self, classifier, param_dist, search_type='random'):\n",
        "        \"\"\"Trains a classifier using GridSearchCV or RandomizedSearchCV with SMOTE.\"\"\"\n",
        "        smote_nc = SMOTENC(categorical_features=[i for i, col in enumerate(self.X_train.columns) if self.X_train[col].nunique() < 10],\n",
        "                            sampling_strategy='minority', random_state=42)\n",
        "\n",
        "        pipeline = make_pipeline(smote_nc, classifier)\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        search = RandomizedSearchCV(pipeline, param_dist, n_iter=10, scoring=\"roc_auc\", n_jobs=-1, cv=cv) if search_type == 'random' else GridSearchCV(pipeline, param_dist, scoring=\"roc_auc\", n_jobs=-1, cv=cv)\n",
        "\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        return search\n",
        "\n",
        "    def evaluate_model(self, model, model_name):\n",
        "        \"\"\"Evaluates the trained model using classification metrics.\"\"\"\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        y_prob = model.predict_proba(self.X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(self.y_test, y_prob)\n",
        "\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(classification_report(self.y_test, y_pred))\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc(fpr, tpr):.2f})')\n",
        "\n",
        "    def train_and_evaluate_models(self):\n",
        "        \"\"\"Trains and evaluates multiple classifiers.\"\"\"\n",
        "        models = {\n",
        "            'Decision Tree': (DecisionTreeClassifier(), {'decisiontreeclassifier__max_depth': [4, 6, 8]}),\n",
        "            'Random Forest': (RandomForestClassifier(), {'randomforestclassifier__n_estimators': [50, 100], 'randomforestclassifier__max_depth': [6, 8]}),\n",
        "            'XGBoost': (XGBClassifier(tree_method='hist'), {'xgbclassifier__max_depth': [6, 8], 'xgbclassifier__learning_rate': [0.05, 0.1]}),\n",
        "            'Neural Network': (MLPClassifier(random_state=42), {'mlpclassifier__hidden_layer_sizes': [(50,), (100,)]})\n",
        "        }\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        for model_name, (clf, params) in models.items():\n",
        "            trained_model = self.train_classifier(clf, params)\n",
        "            self.evaluate_model(trained_model, model_name)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline = FraudDetectionPipeline(\"fraud_data.csv\")\n",
        "pipeline.load_data()\n",
        "pipeline.preprocess_data()\n",
        "pipeline.feature_selection()\n",
        "pipeline.balance_data(method='undersampling')\n",
        "pipeline.train_and_evaluate_models()\n"
      ]
    }
  ]
}